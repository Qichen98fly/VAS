The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:  33%|███▎      | 1/3 [01:28<02:56, 88.14s/it]Downloading shards:  67%|██████▋   | 2/3 [02:56<01:28, 88.14s/it]Downloading shards: 100%|██████████| 3/3 [03:52<00:00, 73.48s/it]Downloading shards: 100%|██████████| 3/3 [03:52<00:00, 77.44s/it]
`LlamaRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:36<01:13, 36.82s/it]